#!/usr/bin/env python
import os
import sys
import json
import argparse
import subprocess
import logging as lg
from os.path import basename, exists

from azkaban import Job, Project
from azkaban.remote import Session


formatter = lg.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s', '%d/%m/%y %H:%M:%S')
ch = lg.StreamHandler(sys.stdout)
ch.setFormatter(formatter)
logger = lg.getLogger('azkaban_submit_job')
logger.setLevel(lg.INFO)
logger.addHandler(ch)

def start_session(url, username, password):
    session = Session(url)
    session.user = username
    session.password = password
    session.id = None
    return session

def run_shell_command(cmd, description, cwd=None, shell=False):
    if not cwd:
        cwd = os.getcwd()
    p = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, shell=shell)
    stdout, stderr = p.communicate()
    logger.info('%s: %s' % (description, stdout))
    if stderr:
        logger.error(stderr)
        exit(-1)
    else:
        return stdout.strip()

def get_camus_version(path):
    path = os.path.join(path, 'camus-shopify')
    logger.info('Retrieving Camus version from %s' % path)
    cmd = "mvn org.apache.maven.plugins:maven-help-plugin:2.1.1:evaluate -Dexpression=project.version | grep -Ev '(^\[|Download\w+:)'"
    return run_shell_command(cmd, 'Retrieved Camus version', path, True)

def main():
    repo_root = os.getcwd()
    camus_jar = get_camus_version(repo_root)
    azkaban_auth = json.load(open(os.path.join(repo_root, 'camus-shopify', 'azkaban.json')))

    camus_command = 'bash -c "curl --compressed -H \'Accept: application/json\' -X GET \'http://resource-manager1.hu131.data-chi.shopify.com:8088/ws/v1/cluster/apps?states=RUNNING,ACCEPTED\' | grep -v Camus && echo \'Camus is not running\' && '
    camus_command += 'java -Xms1G -Xmx2G -DHADOOP_USER_NAME=deploy -Dlog4j.configuration=file:/u/apps/camus/shared/log4j.xml '
    camus_command += '-cp $(hadoop classpath):/u/apps/camus/current/camus-shopify-0.1.0-shopify1.jar:/etc/camus:/etc/hadoop/conf '
    camus_command += 'com.linkedin.camus.etl.kafka.CamusJob -P /u/apps/camus/shared/camus.properties"'

    camus_watermark_cmd = 'bash -c "'
    camus_watermark_cmd += 'java -Xms1G -Xmx2G -DHADOOP_USER_NAME=deploy -Dlog4j.configuration=file:/u/apps/camus/shared/log4j.xml '
    camus_watermark_cmd += '-cp $(hadoop classpath):/u/apps/camus/current/camus-shopify-0.1.0-shopify1.jar:/etc/camus:/etc/hadoop/conf '
    camus_watermark_cmd += 'org.wikimedia.analytics.refinery.job.CamusPartitionChecker -c /u/apps/camus/shared/camus.properties"'

    monitor_cmd = 'bash -c "'
    monitor_cmd += 'java -Xms1G -Xmx2G -DHADOOP_USER_NAME=deploy -Dlog4j.configuration=file:/u/apps/camus/shared/log4j.xml '
    monitor_cmd += '-cp $(hadoop classpath):/u/apps/camus/current/camus-shopify-0.1.0-shopify1.jar:/etc/camus:/etc/hadoop/conf '
    monitor_cmd += 'com.linkedin.camus.shopify.LateArrivingDataMonitor -c /u/apps/camus/shared/camus.properties"'

    hive_partitions_cmd = 'bash -c "'
    hive_partitions_cmd += 'HADOOP_USER_NAME=deploy python /u/apps/camus/shared/hive-generic-partitioner.py %s ' % (os.path.join('hdfs://hadoop-production', 'data', 'raw', 'kafka'))
    hive_partitions_cmd += '--hive-options=\'--auxpath /u/apps/camus/shared/json-serde-1.3.7-jar-with-dependencies.jar\' '
    hive_partitions_cmd += '--database default '
    hive_partitions_cmd += '--verbose"'

    project = Project('Camus')
    project.add_job('Camus Import', Job({'failure.emails': 'camus@shopify.pagerduty.com',
                                         'type': 'command',
                                         'command': camus_command
                                         }))
    project.add_job("Camus Watermark", Job({'failure.emails': 'camus@shopify.pagerduty.com',
                                            'type': 'command',
                                            'command': camus_watermark_cmd
                                            }))
    project.add_job("Late-Arriving-Data Monitor", Job({'type': 'command',
                                                       'command': monitor_cmd
                                                       }))
    project.add_job("Hive Partitions", Job({'type': 'command',
                                            'command': hive_partitions_cmd
                                            }))

    logger.info('Building zip file')
    project.build('camus.zip', True)
    logger.info('Connecting to Azkaban site')
    session = start_session(azkaban_auth['server'], azkaban_auth['user'], azkaban_auth['password'])

    logger.info('Uploading zip file')
    session.upload_project('Camus', 'camus.zip')
    logger.info('Successfully uploaded Camus zip file.')

    session.schedule_workflow('Camus', 'Camus Import',               date='01/01/2015', time="11,30,AM,UTC", period="1h", concurrent=False)
    session.schedule_workflow('Camus', 'Camus Watermark',            date='01/01/2015', time="12,10,AM,UTC", period="1h", concurrent=False)
    session.schedule_workflow('Camus', 'Late-Arriving-Data Monitor', date='01/01/2015', time="12,20,AM,UTC", period="1h", concurrent=False)
    logger.info('Successfully scheduled Camus flow.')

if __name__ == '__main__':
    main()
